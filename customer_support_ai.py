import streamlit as st
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("HUGGINGFACE_API_KEY")

# Hugging Face API ì„¤ì •
client = InferenceClient(provider="hf-inference", api_key=api_key)

# Streamlit UI ì„¤ì •
st.set_page_config(page_title="í—·GPT", page_icon="ğŸ’¬", layout="wide")


with st.sidebar:
    st.header("ğŸ“Œ ì„¤ì •")
    clear_chat = st.button("ğŸ’¬ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”")
    
    
    if clear_chat:
        st.session_state.chat_history = []
        st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")   
    

st.title("ğŸ’¬ ë§ì¶¤í˜• AI ê³ ê° ìƒë‹´ ì±—ë´‡")
st.write("ê³ ê°ë‹˜ì˜ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ í—·GPTê°€ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")

# ëŒ€í™” ê¸°ë¡ ì €ì¥
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# AI ì‘ë‹µ ì²˜ë¦¬ í•¨ìˆ˜
def get_response():
    user_input = st.session_state.chat_input
    if user_input:
        with st.spinner("í—·GPTê°€ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            response = client.chat.completions.create(
                model="google/gemma-2-9b-it", 
                messages=[{"role": "user", "content": user_input}], 
                max_tokens=1024,
            ).choices[0].message.content
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            st.session_state.chat_history.insert(0, ("ğŸ‘¤ ì‚¬ìš©ì:", user_input))
            st.session_state.chat_history.insert(0, ("ğŸ¤– í—·GPT:", response))
            st.session_state.pop("chat_input", None)

# ëŒ€í™” ì¶œë ¥ (ìµœì‹  ë©”ì‹œì§€ê°€ ìœ„ë¡œ)
st.markdown("### ëŒ€í™” ê¸°ë¡")
for role, message in reversed(st.session_state.chat_history):
    st.markdown(f"**{role}** {message}")
# ì…ë ¥ í•„ë“œ
st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="chat_input", on_submit=get_response)
st.write("chat GPT ì•„ë‹ˆê³  í—·GPT ì…ë‹ˆë‹¤.")
