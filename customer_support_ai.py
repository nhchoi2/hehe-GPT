import streamlit as st
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("HUGGINGFACE_API_KEY")

# Hugging Face API ì„¤ì •
client = InferenceClient(provider="hf-inference", api_key=api_key)

# Streamlit UI ì„¤ì •
st.set_page_config(page_title="AI ê³ ê° ìƒë‹´ ì±—ë´‡", page_icon="ğŸ’¬", layout="wide")

st.title("ğŸ’¬ ë§ì¶¤í˜• AI ê³ ê° ìƒë‹´ ì±—ë´‡")
st.write("ê³ ê°ë‹˜ì˜ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")

# ëŒ€í™” ê¸°ë¡ ì €ì¥
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# AI ì‘ë‹µ ì²˜ë¦¬ í•¨ìˆ˜
def get_response():
    user_input = st.session_state.chat_input
    if user_input:
        with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            response = client.chat.completions.create(
                model="google/gemma-2-9b-it", 
                messages=[{"role": "user", "content": user_input}], 
                max_tokens=1024,
            ).choices[0].message.content
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            st.session_state.chat_history.insert(0, ("ğŸ‘¤ ì‚¬ìš©ì:", user_input))
            st.session_state.chat_history.insert(0, ("ğŸ¤– AI:", response))
            st.session_state.pop("chat_input", None)

# ëŒ€í™” ì¶œë ¥ (ìµœì‹  ë©”ì‹œì§€ê°€ ìœ„ë¡œ)
st.markdown("### ëŒ€í™” ê¸°ë¡")
for role, message in reversed(st.session_state.chat_history):
    st.markdown(f"**{role}** {message}")

# ì…ë ¥ í•„ë“œ
st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="chat_input", on_submit=get_response)

st.write("ì´ ì±—ë´‡ì€ ê³ ê° ìƒë‹´ ë° AI ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤. ğŸš€")
