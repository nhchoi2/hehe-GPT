import streamlit as st
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import os
from pinecone import Pinecone, ServerlessSpec

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼)
load_dotenv()
api_key = os.getenv("HUGGINGFACE_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_env = os.getenv("PINECONE_ENV")
pinecone_index_name = os.getenv("PINECONE_INDEX_NAME")
pinecone_cloud = os.getenv("PINECONE_CLOUD")  # ì˜ˆ: 'aws'

# 2. Hugging Face API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# - ì±„íŒ… ì‘ë‹µìš©: google/gemma-2-9b-it ëª¨ë¸
client = InferenceClient(provider="hf-inference", api_key=api_key)
# - ì„ë² ë”© ìƒì„±ìš©: sentence-transformers/all-MiniLM-L6-v2 ëª¨ë¸
embedding_client = InferenceClient(model="sentence-transformers/all-MiniLM-L6-v2", api_key=api_key)

# 3. Pinecone ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìµœì‹  ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©)
pc = Pinecone(api_key=pinecone_api_key)
# Pinecone ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒì„± (all-MiniLM-L6-v2 ì„ë² ë”© ì°¨ì›ì€ 384)
if pinecone_index_name not in [idx.name for idx in pc.list_indexes()]:
    pc.create_index(
        name=pinecone_index_name,
        dimension=384,
        metric='cosine',
        spec=ServerlessSpec(cloud=pinecone_cloud, region=pinecone_env)
    )
# ì¸ë±ìŠ¤ ì—°ê²°
index = pc.Index(pinecone_index_name)

# 4. Streamlit UI ì„¤ì •
st.set_page_config(page_title="ğŸ“œ_ì„¸ë²•_Het", page_icon="ğŸ’¬", layout="wide")

current_page = "ai_tex_assistant"  # í˜„ì¬ í˜ì´ì§€ì˜ ê³ ìœ í•œ ì‹ë³„ì
page_key = f"chat_history_{current_page}"

# í˜ì´ì§€ë³„ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if page_key not in st.session_state:
    st.session_state[page_key] = []

# ì‚¬ì´ë“œë°”: ì„¤ì • ë° ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
with st.sidebar:
    st.header("ğŸ“Œ ì„¤ì •")
    clear_chat = st.button("ğŸ’¬ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”")
    if clear_chat:
        st.session_state[page_key] = []
        st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ íƒ€ì´í‹€ ë° ì„¤ëª…
st.title("ğŸ“œ_ì„¸ë²•_Het")
st.write("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ğŸ“œ_ì„¸ë²•_Hetì´ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")
with st.expander("ğŸ”¹ ì‚¬ìš© ë°©ë²• ì˜ˆì‹œ ë³´ê¸°"):
    st.markdown("""
ì´ í˜ì´ì§€ëŠ” **ì„¸ë¬´ë²•ë ¹ì— ëŒ€í•œ AI ì±—ë´‡**ì…ë‹ˆë‹¤.  
ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì„ **AIê°€ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì„¸ë¬´ ì •ë³´ë¥¼ ì œê³µ**í•©ë‹ˆë‹¤.  
- ğŸ” **Pinecone ë°ì´í„°ë² ì´ìŠ¤**ë¥¼ í™œìš©í•˜ì—¬ ê´€ë ¨ ë²•ë ¹ì„ ê²€ìƒ‰  
- ğŸ¤– **Hugging Face AI ëª¨ë¸(Gemma-2-9B-it)**ì´ ë²•ë ¹ì„ ë¶„ì„í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ì œê³µ  

ğŸ“Œ **ì–´ë–¤ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆë‚˜ìš”?**

âœ… ì†Œë“ì„¸, ë²•ì¸ì„¸, ë¶€ê°€ê°€ì¹˜ì„¸ ê´€ë ¨ ë²•ë ¹  
âœ… ì„¸ë¬´ ê°ë©´ ë° ê³µì œ í˜œíƒ  
âœ… ì‚¬ì—…ì ì„¸ê¸ˆ ì‹ ê³  ì ˆì°¨  
âœ… ì¢…í•©ì†Œë“ì„¸ ì‹ ê³  ê¸°í•œ ë° ì œì¶œ ë°©ë²•
âœ… ê¸°íƒ€ ì„¸ë¬´ ê´€ë ¨ ë²•ë ¹ ë° ì ˆì°¨  

âš ï¸ **ì£¼ì˜:** AIê°€ ì œê³µí•˜ëŠ” ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ ë²•ë¥  ìë¬¸ì´ í•„ìš”í•  ê²½ìš° ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.
""")
def get_embedding(text):
    result = embedding_client.feature_extraction(text)
    # ë§Œì•½ ê²°ê³¼ê°€ numpy ndarrayë¼ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    if hasattr(result, "tolist"):
        result = result.tolist()
    # ë§Œì•½ ë‹¤ì¤‘ í† í° ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ë¼ë©´, ì²« ë²ˆì§¸ í† í°ì˜ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    if isinstance(result, list) and isinstance(result[0], list):
        return result[0]
    return result

def query_pinecone(query_text, top_k=3):
    embedding = get_embedding(query_text)
    if embedding is None:
        return None
    results = index.query(vector=embedding, top_k=top_k, include_metadata=True)
    return results

def generate_prompt(user_input, context):
    system_prompt = """
        ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì„¸ë¬´ì‚¬ ê´€ë¦¬ìì…ë‹ˆë‹¤. í•­ìƒ ì¹œì ˆí•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ì„¸ë¬´ ìƒë‹´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

        - ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì´ ë¶€ì¡±í•˜ë©´ ë°˜ë“œì‹œ "ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì´ ë§ì•„ ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”"ë¼ëŠ” ë©”ì‹œì§€ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.
        - Pineconeì—ì„œ ì ì ˆí•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ì¼ë§ˆ ëª¨ë¸ì˜ LLMì„ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì‹­ì‹œì˜¤.
        - ì‚¬ìš©ìê°€ ì„¸ë¬´ì‚¬ ì¶”ì²œì„ ìš”ì²­í•˜ëŠ” ê²½ìš°, ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•˜ì‹­ì‹œì˜¤:

        **ì¶”ì²œ ì„¸ë¬´ì‚¬**  
        - **ê¶Œë„ìœ¤ ì„¸ë¬´ì‚¬**  
        - **THE KEVIN's TAX LAB**  
        - **ì „í™”: 02-403-0601**
        """
    # ì‚¬ìš©ì ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° ë°”ë¡œ í•´ë‹¹ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    if len(user_input.strip()) < 10:
        return "ì„¸ë²• ê´€ë ¨ ë‚´ìš©ë§Œ ë‹µë³€ì´ ê°€ëŠ¥ í•©ë‹ˆë‹¤. "
    
    prompt = f"{system_prompt}\nContext:\n{context}\n\nQuestion: {user_input}\nAnswer:"
    return prompt

def get_response():
    user_input = st.session_state.chat_input
    if user_input:
        with st.spinner("ì„¸ë²•_Hetì´ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            # Pineconeì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            results = query_pinecone(user_input)
            context = ""
            if results and "matches" in results:
                for match in results["matches"]:
                    context += match["metadata"].get("text", "") + "\n"
            
            # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
            conversation_history = ""
            for role, message in st.session_state[page_key]:
                conversation_history += f"{role} {message}\n"
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨
            system_prompt = """
                ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ í•œêµ­ì¸ ì„¸ë¬´ì‚¬ ê´€ë¦¬ìì…ë‹ˆë‹¤. í•­ìƒ ì¹œì ˆí•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
                ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ì„¸ë¬´ ìƒë‹´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
                ...
            """
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"{system_prompt}\nëŒ€í™” ê¸°ë¡:\n{conversation_history}\nContext:\n{context}\n\nì‚¬ìš©ì: {user_input}\nëª¨ë¸:"
            
            # í”„ë¡¬í”„íŠ¸ê°€ ë‹¨ìˆœí•œ êµ¬ì²´ì„± ë¶€ì¡± ë©”ì‹œì§€ì¸ ê²½ìš°, ëª¨ë¸ í˜¸ì¶œ ì—†ì´ í•´ë‹¹ ë©”ì‹œì§€ë¥¼ ì‘ë‹µìœ¼ë¡œ ì‚¬ìš©
            if user_input.strip() == "":
                response = "ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì´ ë§ì•„ ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            else:
                response = client.chat.completions.create(
                    model="google/gemma-2-9b-it",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=1024,
                ).choices[0].message.content
            
            # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ (ìµœì‹  ë©”ì‹œì§€ê°€ ìœ„ì— í‘œì‹œë˜ë„ë¡)
            st.session_state[page_key] .insert(0, ("ğŸ‘¤ ì‚¬ìš©ì:", user_input))
            st.session_state[page_key] .insert(0, ("ğŸ¤– ì„¸ë²•_Het:", response))
            st.session_state.pop("chat_input", None)

# ëŒ€í™” ê¸°ë¡ ì¶œë ¥ (ìµœì‹  ë©”ì‹œì§€ê°€ ìœ„ìª½ì— ë³´ì´ë„ë¡ ì—­ìˆœ ì¶œë ¥)
st.markdown("### ëŒ€í™” ê¸°ë¡")
for role, message in reversed(st.session_state[page_key]):
    st.markdown(f"**{role}** {message}")

# ì±„íŒ… ì…ë ¥ í•„ë“œ: ì…ë ¥ í›„ get_response í•¨ìˆ˜ í˜¸ì¶œ
st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="chat_input", on_submit=get_response)
