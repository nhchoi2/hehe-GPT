import streamlit as st  # Streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
from huggingface_hub import InferenceClient  # Hugging Face API í´ë¼ì´ì–¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
from dotenv import load_dotenv  # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os  # ìš´ì˜ ì²´ì œ ê´€ë ¨ ê¸°ëŠ¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
api_key = os.getenv("HUGGINGFACE_API_KEY")  # API í‚¤ ê°€ì ¸ì˜¤ê¸°

# Hugging Face API ì„¤ì •
client = InferenceClient(provider="hf-inference", api_key=api_key)  # Hugging Face API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

# Streamlit UI ì„¤ì •
st.set_page_config(page_title="í—·GPT", page_icon="ğŸ’¬", layout="wide")  # í˜ì´ì§€ ì œëª©, ì•„ì´ì½˜ ë° ë ˆì´ì•„ì›ƒ ì„¤ì •

# í˜„ì¬ í˜ì´ì§€ì— ëŒ€í•œ ê³ ìœ í•œ í‚¤ ìƒì„± (í˜ì´ì§€ë³„ ëŒ€í™” ê¸°ë¡ ìœ ì§€)
current_page = "ai_het_assistant"  # í˜„ì¬ í˜ì´ì§€ì˜ ê³ ìœ í•œ ì‹ë³„ì
page_key = f"chat_history_{current_page}"

# í˜ì´ì§€ë³„ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” (ì¤‘ë³µ ì—†ì´ í•œ ë²ˆë§Œ ì´ˆê¸°í™”)
if page_key not in st.session_state:
    st.session_state[page_key] = []

# ì‚¬ì´ë“œë°” ì¶”ê°€
with st.sidebar:
    st.header("ğŸ“Œ ì„¤ì •")  # ì‚¬ì´ë“œë°” í—¤ë”
    clear_chat = st.button("ğŸ’¬ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”")  # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
    if clear_chat:
        st.session_state[page_key] = []  # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
        st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")  # ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥

# ë©”ì¸ ì œëª© ë° ì„¤ëª…
st.title("ğŸ’¬ ë˜‘ë˜‘í•œ AI í—·GPT")  # ë©”ì¸ í˜ì´ì§€ ì œëª©
st.write("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ í—·GPTê°€ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")  # í˜ì´ì§€ ì„¤ëª…

# AI ì‘ë‹µ ì²˜ë¦¬ í•¨ìˆ˜
def get_response():
    # ì˜¬ë°”ë¥¸ ì…ë ¥ ê°’ ê°€ì ¸ì˜¤ê¸° (ì…ë ¥ í•„ë“œëŠ” "chat_input"ì— ì €ì¥ë©ë‹ˆë‹¤)
    user_input = st.session_state.chat_input  
    if user_input:
        # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ì„ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì˜¤ë˜ëœ ìˆœì„œëŒ€ë¡œ)
        # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
            conversation_history = ""
            for role, message in st.session_state[page_key]:
                conversation_history += f"{role} {message}\n"
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨
            system_prompt = """
                ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³ , ì „ë¬¸ì ì¸ í•œêµ­ì¸ ë¹„ì„œì…ë‹ˆë‹¤. í•­ìƒ ì¹œì ˆí•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
                ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
                ...
            """
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"{system_prompt}\nëŒ€í™” ê¸°ë¡:\n{conversation_history}\nContext:\n{context}\n\nì‚¬ìš©ì: {user_input}\nëª¨ë¸:"
                
            with st.spinner("í—·GPTê°€ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                response = client.chat.completions.create(
                    model="google/gemma-2-9b-it",
                    messages=[{"role": "user", "content": prompt}],  # ëŒ€í™” ì´ë ¥ì„ í¬í•¨í•œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
                    max_tokens=1024,
                ).choices[0].message.content
                
                # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ (ìµœì‹  ë©”ì‹œì§€ê°€ ìœ„ì— í‘œì‹œë˜ë„ë¡)
                st.session_state[page_key].insert(0, ("ğŸ‘¤ ì‚¬ìš©ì:", user_input))
                st.session_state[page_key].insert(0, ("ğŸ¤– í—·GPT:", response))
                st.session_state.pop("chat_input", None)
            
# ëŒ€í™” ì¶œë ¥ (ìµœì‹  ë©”ì‹œì§€ê°€ ìœ„ë¡œ)
st.markdown("### ëŒ€í™” ê¸°ë¡")  # ëŒ€í™” ê¸°ë¡ ì„¹ì…˜ ì œëª© ì¶œë ¥
for role, message in reversed(st.session_state[page_key]):  # ëŒ€í™” ê¸°ë¡ì„ ì—­ìˆœìœ¼ë¡œ ì¶œë ¥
    st.markdown(f"**{role}** {message}")

# ì…ë ¥ í•„ë“œ
st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="chat_input", on_submit=get_response)  # ì‚¬ìš©ì ì…ë ¥ í•„ë“œ ì„¤ì • ë° ì‘ë‹µ í•¨ìˆ˜ ì—°ê²°
